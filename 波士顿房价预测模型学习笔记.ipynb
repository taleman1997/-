{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1： 波士顿房价预测模型\n",
    "\n",
    "## 问题描述\n",
    "通过多元线性回归模型，对波士顿放假进行预测。数据集统计了13种可能影响房价的因素和该类型房屋的均价，期望构建一个基于13个因素进行房价预测的模型。 各指标属性名如下`feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS','RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归模型\n",
    "\n",
    "假设房价和各影响因素之间能够用线性关系来描述：\n",
    "\n",
    "$$y = {\\sum_{j=1}^Mx_j w_j} + b$$\n",
    "\n",
    "模型的求解即是通过数据拟合出每个$w_j$和$b$。其中，$w_j$和$b$分别表示该线性模型的权重和偏置。\n",
    "\n",
    "线性回归模型使用均方误差作为损失函数（Loss），用以衡量预测房价和真实房价的差异，公式如下：\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat{Y_i} - {Y_i})^{2}$$\n",
    "注意： 在此处使用均方误差，避免负数的产生，有利于模型的优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建和训练五步骤：\n",
    "**数据处理：** 数据导入，数据形状变换，数据集划分，数据归一化，封装成函数  \n",
    "**模型设计：** 网络结构构建，模型的假设空间，利用模型结构，表达假设关系  \n",
    "**训练配置：** 设定模型采用的寻解方法，指定计算资源  \n",
    "**训练过程：** 循环调用训练过程，每轮都包括向前计算，损失函数，向后传播三个步骤。  \n",
    "**模型保存：** 将训练好的模型进行保存，在模型预测的时候调用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理\n",
    "#### 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "# 读入训练数据\n",
    "datafile = 'housing.data'\n",
    "data = np.fromfile(datafile, sep=' ')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据形状转变\n",
    "以上输入数据为一维 array， 现在需要将每14项分成一条数据，变成 X * 14 的 array 变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', \n",
    "                 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "feature_num = len(feature_names)\n",
    "length = data.shape\n",
    "column_num = int(length[0]/feature_num)\n",
    "\n",
    "data = data.reshape([506, feature_num])\n",
    "# 注意双斜杠是整除的意思\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据集划分\n",
    "在本案例中，我们将80%的数据用作训练集，20%用作测试集，实现代码如下。通过打印训练集的形状，可以发现共有404个样本，每个样本含有13个特征和1个预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "offset = int(data.shape[0] * ratio)\n",
    "training_data = data[:offset]\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据归一化处理\n",
    "\n",
    "对每个特征进行归一化处理，使得每个特征的取值缩放到0~1之间。这样做有两个好处：一是模型训练更高效；二是特征前的权重大小可以代表该变量对预测结果的贡献度（因为每个特征值本身的范围相同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    max_value = training_data.max(axis = 0)\n",
    "    min_value = training_data.min(axis = 0)\n",
    "    ave_value = training_data.mean(axis = 0)\n",
    "    \n",
    "\n",
    "    for i in range (feature_num):\n",
    "        data[:,i] = (data[:, i] - ave_value[i])/(max_value[i] - min_value[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 封装函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def Load_Data():\n",
    "    datafile = 'housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', \n",
    "                    'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 区分训练集和测试集\n",
    "\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "\n",
    "    # 数据归一化\n",
    "    max_value = training_data.max(axis = 0)\n",
    "    min_value = training_data.min(axis = 0)\n",
    "    ave_value = training_data.mean(axis = 0)\n",
    "    \n",
    "\n",
    "    for i in range (feature_num):\n",
    "        data[:,i] = (data[:, i] - ave_value[i])/(max_value[i] - min_value[i])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    training_data = data [:offset]\n",
    "    testing_data =  data [offset:]\n",
    "    return training_data, testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型设计\n",
    "\n",
    "模型设计也hi对网络结构的设计，实现模型向前计算。 在此项目中，输入特种 $x$ 有13个分量，输出$y$仅一个分量，则参数权重形状为$13*1$, 因此参数权重，输入特征，和预测结果存在以下关系  \n",
    "$$z = x*w + b$$  \n",
    "并通过预测结果和实际结果计算损失函数。  \n",
    "可定义一个network类来定义以下函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练过程\n",
    "\n",
    "通过训练，找到使损失函数值较小的参数（$b$,$w$）。可采用随机梯度下降法。  \n",
    "\n",
    "**梯度下降法（英语：Gradient descent）是一个一阶最优化算法，通常也称为最陡下降法，但是不该与近似积分的最陡下降法（英语：Method of steepest descent）混淆。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。**  \n",
    "\n",
    "当函数的梯度下降为零的时候，函数趋向极值  \n",
    "\n",
    "#### 随机梯度下降法（ Stochastic Gradient Descent）\n",
    "\n",
    "在上述程序中，每次损失函数和梯度计算都是基于数据集中的全量数据。对于波士顿房价预测任务数据集而言，样本数比较少，只有404个。但在实际问题中，数据集往往非常大，如果每次都使用全量数据进行计算，效率非常低，通俗地说就是“杀鸡焉用牛刀”。由于参数每次只沿着梯度反方向更新一点点，因此方向并不需要那么精确。一个合理的解决方案是每次从总的数据集中随机抽取出小部分数据来代表整体，基于这部分数据计算梯度和损失来更新参数，这种方法被称作随机梯度下降法（Stochastic Gradient Descent，SGD），核心概念如下：\n",
    "\n",
    "**随机的原因：** 模型会对最后训练的数据有加深记忆，如果样本发生与时间轴有关需要另外考虑。\n",
    "\n",
    "* min-batch：每次迭代时抽取出来的一批数据被称为一个min-batch。\n",
    "* batch_size：一个mini-batch所包含的样本数目称为batch_size。\n",
    "* epoch：当程序迭代的时候，按mini-batch逐渐抽取出样本，当把整个数据集都遍历到了的时候，则完成了一轮训练，也叫一个epoch。启动训练时，可以将训练的轮数num_epochs和batch_size作为参数传入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch   0 / iter   0, loss = 0.2948\n",
      "Epoch   0 / iter   1, loss = 0.2892\n",
      "Epoch   0 / iter   2, loss = 0.2995\n",
      "Epoch   0 / iter   3, loss = 0.3779\n",
      "Epoch   0 / iter   4, loss = 0.6146\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch   1 / iter   0, loss = 0.3129\n",
      "Epoch   1 / iter   1, loss = 0.2351\n",
      "Epoch   1 / iter   2, loss = 0.2831\n",
      "Epoch   1 / iter   3, loss = 0.2426\n",
      "Epoch   1 / iter   4, loss = 0.0872\n",
      "<enumerate object at 0x0000029570DCF728>\n",
      "Epoch   2 / iter   0, loss = 0.2392\n",
      "Epoch   2 / iter   1, loss = 0.2329\n",
      "Epoch   2 / iter   2, loss = 0.3048\n",
      "Epoch   2 / iter   3, loss = 0.1769\n",
      "Epoch   2 / iter   4, loss = 0.2686\n",
      "<enumerate object at 0x0000029570DCF3B8>\n",
      "Epoch   3 / iter   0, loss = 0.2558\n",
      "Epoch   3 / iter   1, loss = 0.2413\n",
      "Epoch   3 / iter   2, loss = 0.1872\n",
      "Epoch   3 / iter   3, loss = 0.1861\n",
      "Epoch   3 / iter   4, loss = 0.2442\n",
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch   4 / iter   0, loss = 0.1860\n",
      "Epoch   4 / iter   1, loss = 0.1754\n",
      "Epoch   4 / iter   2, loss = 0.2022\n",
      "Epoch   4 / iter   3, loss = 0.2241\n",
      "Epoch   4 / iter   4, loss = 0.1511\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch   5 / iter   0, loss = 0.1929\n",
      "Epoch   5 / iter   1, loss = 0.1754\n",
      "Epoch   5 / iter   2, loss = 0.1835\n",
      "Epoch   5 / iter   3, loss = 0.1555\n",
      "Epoch   5 / iter   4, loss = 0.2104\n",
      "<enumerate object at 0x0000029570DCF728>\n",
      "Epoch   6 / iter   0, loss = 0.1789\n",
      "Epoch   6 / iter   1, loss = 0.1636\n",
      "Epoch   6 / iter   2, loss = 0.1722\n",
      "Epoch   6 / iter   3, loss = 0.1338\n",
      "Epoch   6 / iter   4, loss = 0.2103\n",
      "<enumerate object at 0x0000029570DCF3B8>\n",
      "Epoch   7 / iter   0, loss = 0.1349\n",
      "Epoch   7 / iter   1, loss = 0.1665\n",
      "Epoch   7 / iter   2, loss = 0.1545\n",
      "Epoch   7 / iter   3, loss = 0.1488\n",
      "Epoch   7 / iter   4, loss = 0.1957\n",
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch   8 / iter   0, loss = 0.1268\n",
      "Epoch   8 / iter   1, loss = 0.1729\n",
      "Epoch   8 / iter   2, loss = 0.1474\n",
      "Epoch   8 / iter   3, loss = 0.1207\n",
      "Epoch   8 / iter   4, loss = 0.0895\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch   9 / iter   0, loss = 0.1094\n",
      "Epoch   9 / iter   1, loss = 0.1220\n",
      "Epoch   9 / iter   2, loss = 0.1527\n",
      "Epoch   9 / iter   3, loss = 0.1518\n",
      "Epoch   9 / iter   4, loss = 0.0655\n",
      "<enumerate object at 0x0000029570DCF728>\n",
      "Epoch  10 / iter   0, loss = 0.1309\n",
      "Epoch  10 / iter   1, loss = 0.1214\n",
      "Epoch  10 / iter   2, loss = 0.1081\n",
      "Epoch  10 / iter   3, loss = 0.1414\n",
      "Epoch  10 / iter   4, loss = 0.1443\n",
      "<enumerate object at 0x0000029570DCF3B8>\n",
      "Epoch  11 / iter   0, loss = 0.1091\n",
      "Epoch  11 / iter   1, loss = 0.1360\n",
      "Epoch  11 / iter   2, loss = 0.1231\n",
      "Epoch  11 / iter   3, loss = 0.1061\n",
      "Epoch  11 / iter   4, loss = 0.1519\n",
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch  12 / iter   0, loss = 0.0892\n",
      "Epoch  12 / iter   1, loss = 0.1321\n",
      "Epoch  12 / iter   2, loss = 0.0974\n",
      "Epoch  12 / iter   3, loss = 0.1297\n",
      "Epoch  12 / iter   4, loss = 0.0765\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch  13 / iter   0, loss = 0.1332\n",
      "Epoch  13 / iter   1, loss = 0.1100\n",
      "Epoch  13 / iter   2, loss = 0.0875\n",
      "Epoch  13 / iter   3, loss = 0.0975\n",
      "Epoch  13 / iter   4, loss = 0.0290\n",
      "<enumerate object at 0x0000029570DCF728>\n",
      "Epoch  14 / iter   0, loss = 0.0935\n",
      "Epoch  14 / iter   1, loss = 0.0755\n",
      "Epoch  14 / iter   2, loss = 0.1200\n",
      "Epoch  14 / iter   3, loss = 0.1206\n",
      "Epoch  14 / iter   4, loss = 0.0671\n",
      "<enumerate object at 0x0000029570DCF3B8>\n",
      "Epoch  15 / iter   0, loss = 0.0627\n",
      "Epoch  15 / iter   1, loss = 0.1198\n",
      "Epoch  15 / iter   2, loss = 0.1204\n",
      "Epoch  15 / iter   3, loss = 0.0882\n",
      "Epoch  15 / iter   4, loss = 0.0403\n",
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch  16 / iter   0, loss = 0.0950\n",
      "Epoch  16 / iter   1, loss = 0.0955\n",
      "Epoch  16 / iter   2, loss = 0.0915\n",
      "Epoch  16 / iter   3, loss = 0.0908\n",
      "Epoch  16 / iter   4, loss = 0.0896\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch  17 / iter   0, loss = 0.0864\n",
      "Epoch  17 / iter   1, loss = 0.0832\n",
      "Epoch  17 / iter   2, loss = 0.1127\n",
      "Epoch  17 / iter   3, loss = 0.0784\n",
      "Epoch  17 / iter   4, loss = 0.0245\n",
      "<enumerate object at 0x0000029570DCF728>\n",
      "Epoch  18 / iter   0, loss = 0.0963\n",
      "Epoch  18 / iter   1, loss = 0.0719\n",
      "Epoch  18 / iter   2, loss = 0.0837\n",
      "Epoch  18 / iter   3, loss = 0.0943\n",
      "Epoch  18 / iter   4, loss = 0.0358\n",
      "<enumerate object at 0x0000029570DCF3B8>\n",
      "Epoch  19 / iter   0, loss = 0.0701\n",
      "Epoch  19 / iter   1, loss = 0.0701\n",
      "Epoch  19 / iter   2, loss = 0.0832\n",
      "Epoch  19 / iter   3, loss = 0.1089\n",
      "Epoch  19 / iter   4, loss = 0.0176\n",
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch  20 / iter   0, loss = 0.1086\n",
      "Epoch  20 / iter   1, loss = 0.0742\n",
      "Epoch  20 / iter   2, loss = 0.0656\n",
      "Epoch  20 / iter   3, loss = 0.0709\n",
      "Epoch  20 / iter   4, loss = 0.0919\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch  21 / iter   0, loss = 0.0778\n",
      "Epoch  21 / iter   1, loss = 0.0966\n",
      "Epoch  21 / iter   2, loss = 0.0804\n",
      "Epoch  21 / iter   3, loss = 0.0530\n",
      "Epoch  21 / iter   4, loss = 0.0764\n",
      "<enumerate object at 0x0000029570DCF728>\n",
      "Epoch  22 / iter   0, loss = 0.0746\n",
      "Epoch  22 / iter   1, loss = 0.0762\n",
      "Epoch  22 / iter   2, loss = 0.0712\n",
      "Epoch  22 / iter   3, loss = 0.0754\n",
      "Epoch  22 / iter   4, loss = 0.0599\n",
      "<enumerate object at 0x0000029570DCF3B8>\n",
      "Epoch  23 / iter   0, loss = 0.0580\n",
      "Epoch  23 / iter   1, loss = 0.0676\n",
      "Epoch  23 / iter   2, loss = 0.0847\n",
      "Epoch  23 / iter   3, loss = 0.0780\n",
      "Epoch  23 / iter   4, loss = 0.0359\n",
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch  24 / iter   0, loss = 0.0614\n",
      "Epoch  24 / iter   1, loss = 0.0757\n",
      "Epoch  24 / iter   2, loss = 0.0763\n",
      "Epoch  24 / iter   3, loss = 0.0651\n",
      "Epoch  24 / iter   4, loss = 0.0561\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch  25 / iter   0, loss = 0.0673\n",
      "Epoch  25 / iter   1, loss = 0.0592\n",
      "Epoch  25 / iter   2, loss = 0.0654\n",
      "Epoch  25 / iter   3, loss = 0.0795\n",
      "Epoch  25 / iter   4, loss = 0.0211\n",
      "<enumerate object at 0x0000029570DCF728>\n",
      "Epoch  26 / iter   0, loss = 0.0845\n",
      "Epoch  26 / iter   1, loss = 0.0616\n",
      "Epoch  26 / iter   2, loss = 0.0568\n",
      "Epoch  26 / iter   3, loss = 0.0600\n",
      "Epoch  26 / iter   4, loss = 0.0396\n",
      "<enumerate object at 0x0000029570DCF3B8>\n",
      "Epoch  27 / iter   0, loss = 0.0647\n",
      "Epoch  27 / iter   1, loss = 0.0650\n",
      "Epoch  27 / iter   2, loss = 0.0572\n",
      "Epoch  27 / iter   3, loss = 0.0697\n",
      "Epoch  27 / iter   4, loss = 0.0138\n",
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch  28 / iter   0, loss = 0.0585\n",
      "Epoch  28 / iter   1, loss = 0.0519\n",
      "Epoch  28 / iter   2, loss = 0.0641\n",
      "Epoch  28 / iter   3, loss = 0.0743\n",
      "Epoch  28 / iter   4, loss = 0.0326\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch  29 / iter   0, loss = 0.0464\n",
      "Epoch  29 / iter   1, loss = 0.0617\n",
      "Epoch  29 / iter   2, loss = 0.0807\n",
      "Epoch  29 / iter   3, loss = 0.0526\n",
      "Epoch  29 / iter   4, loss = 0.0599\n",
      "<enumerate object at 0x0000029570DCF728>\n",
      "Epoch  30 / iter   0, loss = 0.0502\n",
      "Epoch  30 / iter   1, loss = 0.0502\n",
      "Epoch  30 / iter   2, loss = 0.0802\n",
      "Epoch  30 / iter   3, loss = 0.0553\n",
      "Epoch  30 / iter   4, loss = 0.0567\n",
      "<enumerate object at 0x0000029570DCF3B8>\n",
      "Epoch  31 / iter   0, loss = 0.0576\n",
      "Epoch  31 / iter   1, loss = 0.0681\n",
      "Epoch  31 / iter   2, loss = 0.0449\n",
      "Epoch  31 / iter   3, loss = 0.0575\n",
      "Epoch  31 / iter   4, loss = 0.0134\n",
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch  32 / iter   0, loss = 0.0627\n",
      "Epoch  32 / iter   1, loss = 0.0342\n",
      "Epoch  32 / iter   2, loss = 0.0729\n",
      "Epoch  32 / iter   3, loss = 0.0524\n",
      "Epoch  32 / iter   4, loss = 0.0194\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch  33 / iter   0, loss = 0.0658\n",
      "Epoch  33 / iter   1, loss = 0.0491\n",
      "Epoch  33 / iter   2, loss = 0.0549\n",
      "Epoch  33 / iter   3, loss = 0.0470\n",
      "Epoch  33 / iter   4, loss = 0.0464\n",
      "<enumerate object at 0x0000029570DCF728>\n",
      "Epoch  34 / iter   0, loss = 0.0802\n",
      "Epoch  34 / iter   1, loss = 0.0469\n",
      "Epoch  34 / iter   2, loss = 0.0335\n",
      "Epoch  34 / iter   3, loss = 0.0479\n",
      "Epoch  34 / iter   4, loss = 0.1198\n",
      "<enumerate object at 0x0000029570DCF3B8>\n",
      "Epoch  35 / iter   0, loss = 0.0568\n",
      "Epoch  35 / iter   1, loss = 0.0470\n",
      "Epoch  35 / iter   2, loss = 0.0692\n",
      "Epoch  35 / iter   3, loss = 0.0335\n",
      "Epoch  35 / iter   4, loss = 0.0469\n",
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch  36 / iter   0, loss = 0.0587\n",
      "Epoch  36 / iter   1, loss = 0.0454\n",
      "Epoch  36 / iter   2, loss = 0.0495\n",
      "Epoch  36 / iter   3, loss = 0.0467\n",
      "Epoch  36 / iter   4, loss = 0.0506\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch  37 / iter   0, loss = 0.0705\n",
      "Epoch  37 / iter   1, loss = 0.0508\n",
      "Epoch  37 / iter   2, loss = 0.0408\n",
      "Epoch  37 / iter   3, loss = 0.0352\n",
      "Epoch  37 / iter   4, loss = 0.0251\n",
      "<enumerate object at 0x0000029570DCF728>\n",
      "Epoch  38 / iter   0, loss = 0.0612\n",
      "Epoch  38 / iter   1, loss = 0.0387\n",
      "Epoch  38 / iter   2, loss = 0.0438\n",
      "Epoch  38 / iter   3, loss = 0.0497\n",
      "Epoch  38 / iter   4, loss = 0.0144\n",
      "<enumerate object at 0x0000029570DCF3B8>\n",
      "Epoch  39 / iter   0, loss = 0.0556\n",
      "Epoch  39 / iter   1, loss = 0.0329\n",
      "Epoch  39 / iter   2, loss = 0.0384\n",
      "Epoch  39 / iter   3, loss = 0.0603\n",
      "Epoch  39 / iter   4, loss = 0.0817\n",
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch  40 / iter   0, loss = 0.0478\n",
      "Epoch  40 / iter   1, loss = 0.0400\n",
      "Epoch  40 / iter   2, loss = 0.0375\n",
      "Epoch  40 / iter   3, loss = 0.0620\n",
      "Epoch  40 / iter   4, loss = 0.0082\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch  41 / iter   0, loss = 0.0412\n",
      "Epoch  41 / iter   1, loss = 0.0404\n",
      "Epoch  41 / iter   2, loss = 0.0582\n",
      "Epoch  41 / iter   3, loss = 0.0439\n",
      "Epoch  41 / iter   4, loss = 0.0115\n",
      "<enumerate object at 0x0000029570DCF728>\n",
      "Epoch  42 / iter   0, loss = 0.0473\n",
      "Epoch  42 / iter   1, loss = 0.0314\n",
      "Epoch  42 / iter   2, loss = 0.0586\n",
      "Epoch  42 / iter   3, loss = 0.0437\n",
      "Epoch  42 / iter   4, loss = 0.0195\n",
      "<enumerate object at 0x0000029570DCF3B8>\n",
      "Epoch  43 / iter   0, loss = 0.0401\n",
      "Epoch  43 / iter   1, loss = 0.0513\n",
      "Epoch  43 / iter   2, loss = 0.0582\n",
      "Epoch  43 / iter   3, loss = 0.0289\n",
      "Epoch  43 / iter   4, loss = 0.0155\n",
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch  44 / iter   0, loss = 0.0350\n",
      "Epoch  44 / iter   1, loss = 0.0541\n",
      "Epoch  44 / iter   2, loss = 0.0392\n",
      "Epoch  44 / iter   3, loss = 0.0482\n",
      "Epoch  44 / iter   4, loss = 0.0165\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch  45 / iter   0, loss = 0.0344\n",
      "Epoch  45 / iter   1, loss = 0.0527\n",
      "Epoch  45 / iter   2, loss = 0.0606\n",
      "Epoch  45 / iter   3, loss = 0.0247\n",
      "Epoch  45 / iter   4, loss = 0.0465\n",
      "<enumerate object at 0x0000029570DCF728>\n",
      "Epoch  46 / iter   0, loss = 0.0445\n",
      "Epoch  46 / iter   1, loss = 0.0536\n",
      "Epoch  46 / iter   2, loss = 0.0484\n",
      "Epoch  46 / iter   3, loss = 0.0252\n",
      "Epoch  46 / iter   4, loss = 0.0232\n",
      "<enumerate object at 0x0000029570DCF3B8>\n",
      "Epoch  47 / iter   0, loss = 0.0307\n",
      "Epoch  47 / iter   1, loss = 0.0607\n",
      "Epoch  47 / iter   2, loss = 0.0493\n",
      "Epoch  47 / iter   3, loss = 0.0273\n",
      "Epoch  47 / iter   4, loss = 0.0272\n",
      "<enumerate object at 0x0000029570DCF408>\n",
      "Epoch  48 / iter   0, loss = 0.0323\n",
      "Epoch  48 / iter   1, loss = 0.0337\n",
      "Epoch  48 / iter   2, loss = 0.0567\n",
      "Epoch  48 / iter   3, loss = 0.0411\n",
      "Epoch  48 / iter   4, loss = 0.0672\n",
      "<enumerate object at 0x0000029570DCF368>\n",
      "Epoch  49 / iter   0, loss = 0.0287\n",
      "Epoch  49 / iter   1, loss = 0.0486\n",
      "Epoch  49 / iter   2, loss = 0.0449\n",
      "Epoch  49 / iter   3, loss = 0.0402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  49 / iter   4, loss = 0.0149\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8a16eb056186>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mplot_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[0mplot_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        #np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        N = x.shape[0]\n",
    "        gradient_w = 1. / N * np.sum((z-y) * x, axis=0) \n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = 1. / N * np.sum(z-y)\n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w, gradient_b, eta = 0.01): #\n",
    "        self.w = self.w - eta * gradient_w \n",
    "        self.b = self.b - eta * gradient_b\n",
    "            \n",
    "                \n",
    "    def train(self, training_data, num_epoches, batch_size=10, eta=0.01):\n",
    "        n = len(training_data)\n",
    "        losses = []\n",
    "        for epoch_id in range(num_epoches):\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\n",
    "            # 然后再按每次取batch_size条数据的方式取出\n",
    "            np.random.shuffle(training_data)\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "            \n",
    "            for iter_id, mini_batch in enumerate(mini_batches): # enumerate 函数用法，iter_id 为int \n",
    "                #print(self.w.shape)\n",
    "                #print(self.b)\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                a = self.forward(x)\n",
    "                loss = self.loss(a, y)\n",
    "                gradient_w, gradient_b = self.gradient(x, y)\n",
    "                self.update(gradient_w, gradient_b, eta)\n",
    "                losses.append(loss)\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\n",
    "                                 format(epoch_id, iter_id, loss))\n",
    "        \n",
    "        return losses\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = Load_Data()\n",
    "\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "# 启动训练\n",
    "losses = net.train(train_data, num_epoches=50, batch_size=100, eta=0.1)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(len(losses))\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察上述Loss的变化，随机梯度下降加快了训练过程，但由于每次仅基于少量样本更新参数和计算损失，所以损失下降曲线会出现震荡。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "由于房价预测的数据量过少，所以难以感受到随机梯度下降带来的性能提升。\n",
    "\n",
    "------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
